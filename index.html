---
layout: default
title: "OUHCIR: About"
---
<div class="row">
	<div class="1u"></div>
	<div class="7u 12u(mobile) important(mobile)">

		<div class="content content-left">

			<!-- Content -->

			<article class="box page-content">
				<section>
					<span class="image medium"><img src="images/lab/hcir_wordcloud.jpg" alt="" /></span>
					<p>
						At the <b>OU <strong>H</strong>uman-<strong>C</strong>omputer <strong>I</strong>nteraction and <strong>R</strong>ecommendation</b> (HCIR) Lab, we are working toward modeling and supporting people’s problem-solving and decision-making activities with intelligent information search and recommender systems, and understanding the economic, societal, and ethical impacts of advanced search and recommendation algorithms. 
					</p>
					<p>
						Our long-term <b>research goal</b> is to provide useful, timely, fair, and responsible information support for people from diverse backgrounds and communities. Our methods include computational approaches such as data/text mining, deep learning, and natural language processing, as well as qualitative, participatory, and user study research methods. Topics of interest include user biases in information interactions, intelligent information retrieval, bias-aware recommender systems, and adaptive information system evaluation.
					</p>
					<p>
						The <b>Principal Investigator</b>, <a href="https://jiqunl.github.io/me/", target="_blank"><strong>Dr. Jiqun Liu</strong></a>, started this lab in 2020. Dr. Liu is an interdisciplinary information science researcher by training, with research interests, including information retrieval, recommender system, bounded rationality, and ethics in human-computer interaction.
					</p>
				</section>

				<section>
					<a name="news"></a><h3 class="meta">Lab News</h3>
					<ul class="meta">
						<li class="icon fa-clock-o">2025
							<desc><b>New grant:</b> Faculty Investment Program (FIP) Award: Making mistakes like humans: Detecting biased judgments of artificial intelligence, University of Oklahoma Office of the Vice President for Research and Partnerships ($15,000, PI: Jiqun Liu)
							</desc>
							<br><br>
						</li>
						<li class="icon fa-clock-o">2024
							<desc><b>Award:</b> Jamshed Karimnazarov received Undergraduate Research Opportunities Program award. Congratulations!
							</desc>
							<br><br>
							<desc><b>Award:</b> The research proposal titled “Enhancing AI Literacy through Maker-based Learning with Generative AI”, co-authored by Dr. Yong Ju Jung and Dr. Jiqun Liu, has received the Elfreda A. Chatman Research Award 2024 from ASIS&T SIG USE.
							</desc>
							<br><br>
							<desc><b>New grant:</b> Project titled “Functional fixedness evaluation in human-large language model (LLM) interaction” received funding support from Microsoft Research ($20,000, PI: Jiqun Liu)
							</desc>
							<br><br>
						</li>
						<li class="icon fa-clock-o">2023
							<desc><b>New grant:</b> PI Liu’s proposal “identifying and mitigating cognitive biases in Generative-AI-Assisted Online Learning” received funding support ($33,918) from OU VPRP Bridge Fund Investment Program.
							</desc>
							<br><br>
							<desc>
								<b>Microsoft Workshop:</b> We presented our early work on Human-LLM Interactions in the <a href="https://ir-ai.github.io/" target="_blank">Workshop of Task Focused IR in the Era of Generative AI</a> at Microsoft Research, Redmond, Washington (<b>Author: Ben Wang, Jiqun Liu, and Jamshed Karimnazarov</b>, all from the University of Oklahoma, OK, USA).
							</desc>	
							<br><br>	
							<desc>
								<b>ASIST 2023:</b> Our paper “Characterizing and Early Predicting User Performance for Adaptive Search Path Recommendation” wins the <b>Best Information Behavior Conference Paper Award </b> from SIG-USE. (<b>Author: Ben Wang and Jiqun Liu</b>, all from the University of Oklahoma, OK, USA).
							</desc>	
							<br><br>	
							<desc>
								<b>ASIST 2023:</b> Our Ph.D. student Ben Wang has been selected for a <a href="https://www.asist.org/am23/2023-annual-meeting-reviewers/" target="_blank">best poster reviewer award</a>!
							</desc>	
							<br><br>	
							<desc>
								<b>ASIST New Leaders Program:</b> Our Ph.D. student Ben Wang has been accepted into the ASIS&T 2023-2024 class of <a href="https://www.asist.org/programs-services/new-leaders-program/", target="_blank">New Leaders Program</a>!
							</desc>	
							<br><br>					
							<desc>
								<b>Microsoft for Startups:</b> Our lab has be accepted by <a href="https://www.microsoft.com/en-us/startups", target="_blank">Microsoft for Startups Founders Hub</a>.
							</desc>
							<br>
						</li>
						<li class="icon fa-clock-o">2022
							<desc>
								<b>ASIST 2022:</b> Our poster “Children's Interest, Search, and Knowledge: A Pilot Analysis of a STEM Maker Workshop” wins the <b>Best Information Behavior Conference Poster Award </b> from SIG-USE. (<b>Author: Yong Ju Jung and Jiqun Liu</b>, all from the University of Oklahoma, OK, USA).
							</desc>	
							<br><br>					
							<desc>
								<b>FIP 2022:</b> Our project “Toward Expectation-based Effectiveness Metrics for Adaptive Whole Session Information Retrieval Evaluation” is funded by the <b>Faculty Investment Program Award</b> from the University of Oklahoma Office of the Vice President for Research and Partnerships. ($15,000, <b>PI: Jiqun Liu</b>).
							</desc>
							<br>
						</li>

					</ul>

				</section>

				<section>
					<a name="announcement"></a><h3><strong>Join Us</strong></h3>
					<p>
						We are actively looking for self-motivated students to join the lab and work on interesting cutting-edge problems in HCIR-related topics.
						Research opportunities are available at both undergraduate and graduate levels. We are especially interested in students with any of the following backgrounds:
						<ul class="default">
							<li>Human-Computer Interaction, Interactive Information Seeking/Retrieval, Cognitive Psychology or Experimental Economics using quantitative or qualitative methods (or both);</li>
							<li>Machine Learning, Natural Language Processing (NLP), or Text/Data Mining.</li>
						</ul>
						If you are interested, please email me at (jiqunliu@ou.edu) with your CV and a brief description of your previous research experiences & current research interests.
					</p>

					<!-- <a href="all-updates.html" class="button alt">More Updates</a> -->
				</section>

			</article>
		</div>
	</div>

	<div class="3u 12u(mobile)">
		<div class="sidebar">

			<!-- Sidebar -->

			<!-- Recent Posts -->
			<section>
				<ul class="divided">
					<li>
						<article class="box post-summary">
							<h4>Recent Publications</h4>
							<p>
								Accepted in TOIS: Understanding user behavior and measuring system vulnerability.
								<br/><a href="https://arxiv.org/abs/2403.18462", target="_blank">Preprint</a>
							</p>
							<p>
								Accepted in SIGIR: Boundedly rational searchers interacting with medical misinformation: Characterizing context-dependent decoy effects on credibility and usefulness evaluation in sessions.
								<br/><a href="", target="_blank"></a>
							</p>
						</article>
					</li>
					<li>
						<article class="box post-summary">
							<h3><a href="#">Contact</a></h3>
							<p>
								<a href="https://jiqunl.github.io/me/", target="_blank"><strong>Jiqun Liu</strong></a>, Ph.D.<br/>
								<a href="https://www.ou.edu/cas/slis", target="_blank">School of Library and Information Studies</a><br/>
								<a href="https://www.ou.edu/", target="_blank">University of Oklahoma</a><br/>
								<ul>
									<li class="icon fa-envelope-o"><span><a href="mailto:jiqunliu@ou.com">jiqunliu@ou.edu</a></span></li>
									<li class="icon fa-twitter"><span><a href="https://twitter.com/JiqunL?s=20", target="_blank">@JiqunL</a></span></li>
									<li class="icon fa-fax"><span>405-325-3921</span></li>
									<li class="icon fa-github"><span><a href="https://github.com/OUHCIRGroup", target="_blank">OUHCIR Lab</a></span></li>
								</ul>																
							</p>
						</article>
					</li>
				</ul>												
			</section>
		</div>
	</div>
</div>
